{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Graph based Methods for Text Summarization</h1>\n",
    "<br>Darragh Clabby\n",
    "<br>HDAIML_SEP\n",
    "<br>20142935\n",
    "<br>National College of Ireland\n",
    "<br>Dublin, Ireland\n",
    "<br>x20142935@student.ncirl.ie\n",
    "<br>\n",
    "<h2>Abstract</h2>\n",
    "This project explores the application of the\n",
    "TextRank algorithm to the task of automatic text\n",
    "summarization. The TextRank algorithm ranks the importance\n",
    "of sentences in a text by representing the text as a graph, with\n",
    "vertices representing sentences and edges representing\n",
    "similarity between sentences. The texts to be summarized were\n",
    "news articles taken from the Daily Mail data set. Each text\n",
    "contained in the data set is accompanied by a human generated\n",
    "summary which may be used as reference summary against\n",
    "which automatically generated summaries may be assessed.\n",
    "Summarization was implemented according to five methods,\n",
    "three of which employed the TextRank algorithm. The aim of\n",
    "this work was to assess how the basic TextRank method\n",
    "compared to the simpler TF-IDF method; and to assess\n",
    "whether sentence embedding and unsupervised classification\n",
    "could be employed to further improve performance. The\n",
    "results suggest that the basic TextRank performed best (highest\n",
    "accuracy, lowest computational demand). The methods\n",
    "employing sentence embedding and unsupervised classification\n",
    "did not deliver further performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Import & Clean Data</h2>\n",
    "<!--<h3>Import BBC Data</h3>-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_DailyMail_Data(fileList, folderName=\"\"):\n",
    "    summary = []\n",
    "    text = []\n",
    "    nLinesSummary = []\n",
    "    for iFile in fileList:\n",
    "        \n",
    "        tmpFile = open(folderName + \"/\" + iFile, 'r')\n",
    "        rawText = tmpFile.readlines()\n",
    "        tmpFile.close()\n",
    "        \n",
    "        strippedText = \"\"\n",
    "        for t in rawText:\n",
    "            strippedText += t\n",
    "        \n",
    "        strippedText = strippedText.split(\"@highlight\") #separate highlights from the main article by splitting on \"@highlight\"\n",
    "        nLinesSummary.append(len(strippedText) - 1)\n",
    "        textTmp = strippedText[0] # article is the first element in the split text\n",
    "        summaryTmp = \"\" # initialize string for the summary\n",
    "        for h in strippedText[1:]: # iterate through the highlights (all elements of the split text other than the first)\n",
    "            summaryTmp += h + \". \" # append highlights to the summary string, & add full stop and space between highlights\n",
    "        \n",
    "        summary.append(summaryTmp)\n",
    "        text.append(textTmp)\n",
    "    \n",
    "    df = pd.DataFrame({\"Source\": fileList, \"Summary\":summary, \"Text\": text, \"nLinesSummary\": nLinesSummary})\n",
    "    return df\n",
    "\n",
    "def cleanCorpus(textSeries, removeStopWords=True):\n",
    "    cleanText = []\n",
    "    rawText = []\n",
    "    for text in textSeries:\n",
    "        cleanList, rawList = cleanDocument(text, removeStopWords)\n",
    "        cleanText.append(cleanList)\n",
    "        rawText.append(rawList)\n",
    "    return cleanText, rawText\n",
    "\n",
    "def cleanDocument(rawText, removeStopWords=True):\n",
    "    \"\"\"\n",
    "    will return:\n",
    "    - clean list: [ [sentence], [[word], [word]] ]\n",
    "    - raw list: [[sentence], [sentence]]\n",
    "    \"\"\"\n",
    "    \n",
    "    rawText = rawText.replace(\"\\n\", \" .\").replace(\"\\xa0\", \" .\") # replace end of line tage with \" .\"\n",
    "    rawText = nltk.tokenize.sent_tokenize(rawText)\n",
    "        \n",
    "    tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "    cleanList = []\n",
    "    rawList = []\n",
    "    for sentence in rawText:\n",
    "        sentence = sentence.lstrip(\".\")\n",
    "        cleanSentence = sentence.translate(tr) # remove punctuation from string\n",
    "        cleanSentence = cleanSentence.translate(remove_digits) # remove numbers\n",
    "        cleanSentence = cleanSentence.lower() # convert all characters to lower\n",
    "        if removeStopWords:\n",
    "            cleanSentence = [s for s in nltk.tokenize.word_tokenize(cleanSentence) if s not in stop_words] # remove stop words & split sentence into words\n",
    "        else:            \n",
    "            cleanSentence = [s for s in nltk.tokenize.word_tokenize(cleanSentence)] \n",
    "        if len(cleanSentence):\n",
    "            cleanList.append(cleanSentence)\n",
    "            rawList.append(sentence)\n",
    "    \n",
    "    return cleanList, rawList\n",
    "\n",
    "def printLines(rawSentences, iFile):\n",
    "    for i, s in enumerate(rawSentences[iFile]):\n",
    "        print(\"\\nSentence \" + str(i) + \": \" + str(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>TF-IDF</h2>\n",
    "<h3>Calculate TF-IDF Coefficients</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTFIDFcoeffs(wordTokens):\n",
    "    termCounts = pd.DataFrame()\n",
    "    nDocs = len(wordTokens)\n",
    "    for iDoc in range(0, nDocs):\n",
    "        for sentence in wordTokens[iDoc]:\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    termCounts[word][iDoc] += 1\n",
    "                except:\n",
    "                    termCounts[word] = pd.Series(np.zeros(nDocs).astype(int))\n",
    "                    termCounts[word][iDoc] += 1\n",
    "\n",
    "    nTerms = len(termCounts.columns)\n",
    "\n",
    "    C_i = termCounts.sum(axis=1)\n",
    "    B_j = termCounts.astype(bool).sum(axis=0)\n",
    "\n",
    "    TF =  np.array(termCounts)/ np.transpose(np.tile(C_i, (nTerms, 1)))\n",
    "    IDF = np.log(nDocs/np.array(B_j))\n",
    "    TFIDF = TF*np.tile(IDF, (nDocs, 1))\n",
    "\n",
    "    colNames = [c for c in termCounts]\n",
    "    TF = pd.DataFrame(TF, columns = colNames)\n",
    "    IDF = pd.Series(IDF, index = colNames)\n",
    "    TFIDF = pd.DataFrame(TFIDF, columns = colNames)\n",
    "    return TFIDF, TF, IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculate Sentence Scores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScoresTFIDF(wordTokens, TFIDF, sentenceTokens, docIndices=[]):\n",
    "    \n",
    "    if len(docIndices) == 0:\n",
    "        docIndices = [x for x in range(0,len(cleanWords))]\n",
    "    \n",
    "    scores = []\n",
    "    for iDoc in docIndices:\n",
    "        doc = wordTokens[iDoc]    \n",
    "        docScores = []\n",
    "        for sentence in doc:\n",
    "            sentenceScore = 0\n",
    "            \n",
    "            #if len(sentence) == 0:\n",
    "            #    print(iDoc)\n",
    "            \n",
    "            #for word in sentence:\n",
    "            #    sentenceScore += TFIDF[word][iDoc]\n",
    "            #sentenceScore /= len(sentence)\n",
    "            if len(sentence) > 0:\n",
    "                for word in sentence:\n",
    "                    sentenceScore += TFIDF[word][iDoc]\n",
    "                sentenceScore /= len(sentence)\n",
    "            docScores.append(sentenceScore)\n",
    "        dfTmp = pd.DataFrame({\"rawSentence\": sentenceTokens[iDoc], \"sentenceScore\": docScores})\n",
    "        scores.append(dfTmp)\n",
    "        \n",
    "        #scores.append(docScores)\n",
    "    \n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSummary(sentenceScores, nSentences=3):\n",
    "    summary = []\n",
    "    for doc in sentenceScores:\n",
    "        #avgScore = doc['sentenceScore'].mean()\n",
    "        #summaryData = doc[doc['sentenceScore'] > 1.5*avgScore]\n",
    "        #n = 3#len(doc)//5\n",
    "        summaryData = doc.sort_values(by=['sentenceScore'], ascending = False)\n",
    "        summaryData = summaryData.head(nSentences)\n",
    "        summaryData = summaryData.sort_index()\n",
    "        \n",
    "        summaryTmp = \"\"\n",
    "        for sentence in summaryData['rawSentence']:\n",
    "            summaryTmp += sentence\n",
    "        summary.append(summaryTmp)\n",
    "        #summary.append(summaryData)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>TextRank</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSimilarity(wordTokens, docIndices=[]):#, sentenceTokens, docIndices=[]):\n",
    "    \n",
    "    if len(docIndices) == 0:\n",
    "        docIndices = [x for x in range(0,len(cleanWords))]\n",
    "        \n",
    "    simMatrices = []\n",
    "    for iDoc in docIndices:\n",
    "        doc = wordTokens[iDoc]    \n",
    "        nSentences = len(doc)\n",
    "        denom = np.ones((nSentences, nSentences))\n",
    "        docSim = np.zeros((nSentences, nSentences))\n",
    "        for i, sentence_i in enumerate(doc):\n",
    "            #n_i = len(set(sentence_i)) # number of unique words in ith sentence\n",
    "            n_i = len(sentence_i) # number of words in ith sentence\n",
    "            for j, sentence_j in enumerate(doc):                \n",
    "                #n_j = len(set(sentence_j)) # number of unique words in jth sentence\n",
    "                n_j = len(sentence_j) # number of words in jth sentence\n",
    "                #if i > j: # only need to calculate lower triangular matrix since edges undirected and matrix is symmetrical (similarity i to j = similarity j to i)\n",
    "                if i != j:\n",
    "                    #if n_i > 0 and n_j > 0:\n",
    "                    denom[i, j] = np.log(n_i) + np.log(n_j)\n",
    "                    if denom[i, j] > 0:\n",
    "                        docSim[i, j] = len(set(sentence_i).intersection(set(sentence_j)))/denom[i, j]\n",
    "        simMatrices.append(docSim)\n",
    "    return simMatrices   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScoresTextRank(similarityMatrix, rawSentences, d=0.85, initialScores=1, tol = 0.0001, docIndices=[]):\n",
    "    \n",
    "    textRankScores = []\n",
    "    for iDoc, simMat in enumerate(similarityMatrix):\n",
    "        simMat = np.abs(simMat) #NOTE: cosine similarity has a range -1 to 1 whereas text rank expects similarity in the range 0 to 1\n",
    "        nSentences = np.shape(simMat)[0]\n",
    "        sentenceScores = np.ones((nSentences))*initialScores\n",
    "        prev = np.zeros((nSentences))\n",
    "\n",
    "        #N = 30\n",
    "        #error = []\n",
    "        #for n in range(0,N):\n",
    "        error = 1\n",
    "        while error > tol:\n",
    "            for i, score_i in enumerate(sentenceScores):\n",
    "                tmp = 0\n",
    "                for j, score_j in enumerate(sentenceScores):\n",
    "                    w_ji = simMat[j, i]\n",
    "                    w_jk = np.sum(simMat[j])\n",
    "                    #if w_jk > 0:\n",
    "                    if w_jk != 0:\n",
    "                        tmp += w_ji*score_j/w_jk\n",
    "\n",
    "                sentenceScores[i] = (1-d) + d*tmp\n",
    "            #error.append(np.sum(np.abs(sentenceScores[i] - prev)))\n",
    "            error = np.sum(np.abs(sentenceScores[i] - prev))\n",
    "            prev = sentenceScores[i]\n",
    "        #if len(rawSentences) != len(sentenceScores):\n",
    "        #    print(\"sentence length: \" + str(len(rawSentences)) + \"; \\nscore length: \" + str(len(sentenceScores)) + \"\\n\")\n",
    "        #    return []\n",
    "        df = pd.DataFrame({\"rawSentence\": rawSentences[iDoc], \"sentenceScore\": sentenceScores})\n",
    "        #return df\n",
    "    \n",
    "        textRankScores.append(df)\n",
    "    return textRankScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Sentence Embeddings</h2>\n",
    "<h3>Import Sentence Transformer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dclabby/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# from: https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/\n",
    "#!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#sbert_model = SentenceTransformer('bert-base-nli-mean-tokens') # why was this model chosen? see https://www.sbert.net/docs/pretrained_models.html\n",
    "sbert_model = SentenceTransformer('stsb-roberta-base') # optimized for Semantic Textual Similarity (STS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transform Sentences & Calculate Similarity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(u, v):\n",
    "    return (np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))).item()\n",
    "\n",
    "\n",
    "def generateSentenceVecs(sentenceTokens, sbert_model, docIndices=[]):\n",
    "    if len(docIndices) == 0:\n",
    "        docIndices = [x for x in range(0,len(cleanWords))]\n",
    "        \n",
    "    sentenceVectors = []\n",
    "    for iDoc in docIndices:\n",
    "        doc = sentenceTokens[iDoc]\n",
    "        sentenceVectors.append(sbert_model.encode(doc))\n",
    "    return sentenceVectors\n",
    "\n",
    "\n",
    "#def sentenceEmbeddingSimilarity(sentenceTokens, sbert_model, docIndices=[]):\n",
    "def sentenceEmbeddingSimilarity(docVecs, docIndices=[]):\n",
    "    \n",
    "    if len(docIndices) == 0:\n",
    "        docIndices = [x for x in range(0,len(cleanWords))]\n",
    "        \n",
    "    simMatrices = []\n",
    "    for iDoc in docIndices:\n",
    "    #    doc = sentenceTokens[iDoc]\n",
    "    #    sentenceVectors = sbert_model.encode(doc)\n",
    "        sentenceVectors = docVecs[iDoc]\n",
    "                \n",
    "        #nSentences = len(doc)\n",
    "        nSentences = len(sentenceVectors)\n",
    "        docSim = np.zeros((nSentences, nSentences))\n",
    "        for i, vec_i in enumerate(sentenceVectors):\n",
    "            for j, vec_j in enumerate(sentenceVectors):\n",
    "                if i != j:\n",
    "                    docSim[i, j] = cosineSimilarity(vec_i, vec_j)\n",
    "        \n",
    "        simMatrices.append(docSim)\n",
    "    return simMatrices   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>K-Nearest Neighbours</h2>\n",
    "<h3>KNN Algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data, k):\n",
    "    \"\"\"\n",
    "    data is an array with:\n",
    "    - dimension 0 (rows) containing sample points;\n",
    "    - dimension 1 (columns) containing dimensions\n",
    "    - example: data of size n x d contains n data points, each of dimension d\n",
    "    - for a given document, this will take sentence vectors where n is the number of sentences & d is the size of the vector\n",
    "    \"\"\"\n",
    "    nSamples = np.shape(data)[0]\n",
    "    D = np.shape(data)[1]\n",
    "    \n",
    "    #assign random starting positions\n",
    "    i_z = np.random.choice(nSamples, k, replace=False)\n",
    "    z_k = data[i_z, :]\n",
    "    z_initial = np.array(z_k, copy=True)\n",
    "    \n",
    "    # iteratively assign data to nearest z\n",
    "    continueLoop = True # initialize variable to terminate while loop\n",
    "    prevClusterNos = np.zeros(nSamples) # initialize array of zeros to store previous cluster assignments\n",
    "    n = 0 # initialize loop counter\n",
    "    maxIts = 2*nSamples # define maximum iterations to prevent infinite loop\n",
    "    while continueLoop and n < maxIts:    \n",
    "        n += 1 # increment the loop counter\n",
    "        cost = np.zeros([nSamples, k]) # initialize the cost matrix\n",
    "        for i, z in enumerate(z_k): # iterate through each cluster...\n",
    "            cost[:, i] = np.linalg.norm(data - z_k[i],axis=1) # calculate the distance between each data point and the ith cluster centre\n",
    "        clusterNos = np.argmin(cost,axis=1) # identify the closest cluster to each data point\n",
    "        clusterData = []\n",
    "        for i_k in range(0,k): # iterate through each cluster\n",
    "            clusterData.append(data[clusterNos == i_k, :]) # extract the data for the ith cluster\n",
    "            z_k[i_k, :] =  np.mean(clusterData[-1], axis=0) # update the centre of the cluster based on the average of its data\n",
    "        continueLoop = not np.all(clusterNos == prevClusterNos) # continue the loop if the present and previous cluster assignments differ\n",
    "        prevClusterNos = clusterNos # update previous cluster numbers (for the next iteration) based on this iteration's cluster numbers\n",
    "        \n",
    "    return clusterData, clusterNos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h3>Clustering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcClusterCentres(docVecs, rawSentences, k):\n",
    "\n",
    "    clusterCentreScores = []\n",
    "    for sentenceVecs, sentencesRaw in zip(docVecs, rawSentences):\n",
    "        sentenceClusters, clusterNos = knn(sentenceVecs, k)\n",
    "        clusterDicts = []\n",
    "        for i_c, cluster in enumerate(sentenceClusters):\n",
    "            clusterMean = np.mean(cluster,axis=0)\n",
    "            clusterScores = []\n",
    "            for u in cluster:\n",
    "                clusterScores.append(cosineSimilarity(u, clusterMean)) # cosine similarity is not sensitive to sentence size\n",
    "                #clusterScores.append(np.linalg.norm(u - clusterMean)) \n",
    "\n",
    "            clusterSentences = []\n",
    "            for s, c in zip(sentencesRaw, clusterNos):\n",
    "                if c == i_c:\n",
    "                    clusterSentences.append(s)\n",
    "\n",
    "            clusterDicts.append(pd.DataFrame({\"rawSentence\": clusterSentences, \"sentenceScore\": clusterScores}))\n",
    "        clusterCentreScores.append(clusterDicts)\n",
    "    return clusterCentreScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClusterSummaries(clusterCentreScores, nSentences=3):\n",
    "    docSummary = []\n",
    "    for docCluster in clusterCentreScores:\n",
    "        nClusters = len(docCluster)\n",
    "        sentencesPerCluster = [len(s) for s in docCluster]\n",
    "        sentencesPerCluster = np.round((sentencesPerCluster/np.sum(sentencesPerCluster))*nSentences).astype('int')\n",
    "        #summary = []\n",
    "        summaryTmp = \"\"\n",
    "        for clusterData, nC in zip(docCluster, sentencesPerCluster):\n",
    "            if nC > 0:\n",
    "                                \n",
    "                summaryData = clusterData.sort_values(by=['sentenceScore'], ascending = False)\n",
    "                summaryData = summaryData.head(nC)\n",
    "                summaryData = summaryData.sort_index()\n",
    "\n",
    "                #summaryTmp = \"\"\n",
    "                for sentence in summaryData['rawSentence']:\n",
    "                    summaryTmp += sentence\n",
    "            #summary.append(summaryTmp)\n",
    "                \n",
    "        docSummary.append(summaryTmp)\n",
    "        #docSummary.append(summary)\n",
    "    return docSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h3>TextRank Clustering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcClusterRank(textRankScores, clusterCentres):\n",
    "    clusterRank = []\n",
    "    for tr, cc in zip(textRankScores, clusterCentres):\n",
    "        clusterRankTmp = []\n",
    "        for c in cc:\n",
    "            clusterRankTmp.append(pd.merge(tr, c, on=\"rawSentence\").rename(columns={\"sentenceScore_x\": \"sentenceScore\"}))\n",
    "        clusterRank.append(clusterRankTmp)\n",
    "    return clusterRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Evaluate Summaries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRogue(candidate, reference, removeStopWords=True):\n",
    "    rogueScores = []\n",
    "    for c, r in zip(candidate, reference):\n",
    "        candidateClean, candidateRaw = cleanDocument(c, removeStopWords)\n",
    "        referenceClean, referenceRaw = cleanDocument(r, removeStopWords)\n",
    "\n",
    "        candidateSet = set(sum(candidateClean, []))\n",
    "        referenceSet = set(sum(referenceClean, []))\n",
    "        rogueScores.append(len(candidateSet.intersection(referenceSet))/len(referenceSet))\n",
    "    return rogueScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Implement Tests</h2>\n",
    "<h3>0. Load Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0. Load Data\n",
    "#folderName = \"/home/dclabby/Documents/Springboard/HDAIML_SEP/Semester02/ArtIntel/Project/TextSummCode/data\"\n",
    "folderName = \"./data\"\n",
    "folderContents = os.listdir(folderName)\n",
    "nFiles = 10\n",
    "files = folderContents[0:nFiles]\n",
    "dmData = load_DailyMail_Data(files, folderName)\n",
    "cleanWords, rawSentences = cleanCorpus(dmData[\"Text\"])\n",
    "ref = dmData[\"Summary\"]\n",
    "N = np.round(dmData[\"nLinesSummary\"].mean()).astype('int').item()\n",
    "#r = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h3>1. Main</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TF-IDF...\n",
      "Calculating TextRank...\n",
      "Generating Sentence Embeddings...\n",
      "Sentence Embeddings completed in 19.47514319419861s\n",
      "Calculating CosRank...\n",
      "Calculating Cluster Centres...\n",
      "Calculating ClusterRank...\n"
     ]
    }
   ],
   "source": [
    "rogueScore = pd.DataFrame()\n",
    "execTime = []\n",
    "wordCountRatio = []\n",
    "rLength = np.array([len(d.split(\" \")) for d in ref])\n",
    "\n",
    "## 1. TF-IDF\n",
    "print(\"Calculating TF-IDF...\")\n",
    "t1 = time.time()\n",
    "tfidf, tf, idf = calcTFIDFcoeffs(cleanWords)\n",
    "sentenceScores = calcScoresTFIDF(cleanWords, tfidf, rawSentences)\n",
    "tfIdfSummaries = generateSummary(sentenceScores, N)\n",
    "\n",
    "rogueScore[\"TF-IDF\"] = calcRogue(tfIdfSummaries, ref)\n",
    "execTime.append(time.time() - t1)\n",
    "#wordCountRatio.append(np.mean(rLength/np.array([len(d.split(\" \")) for d in tfIdfSummaries])))\n",
    "wordCountRatio.append(rLength/np.array([len(d.split(\" \")) for d in tfIdfSummaries]))\n",
    "\n",
    "## 2. TextRank\n",
    "print(\"Calculating TextRank...\")\n",
    "t1 = time.time()\n",
    "trSimilarity = sentenceSimilarity(cleanWords)#, sentenceTokens)\n",
    "textRankScores = calcScoresTextRank(trSimilarity, rawSentences)\n",
    "textRankSummaries = generateSummary(textRankScores, N)\n",
    "\n",
    "rogueScore[\"TextRank\"] = calcRogue(textRankSummaries, ref)\n",
    "execTime.append(time.time() - t1)\n",
    "#wordCountRatio.append(np.mean(rLength/np.array([len(d.split(\" \")) for d in textRankSummaries])))\n",
    "wordCountRatio.append(rLength/np.array([len(d.split(\" \")) for d in textRankSummaries]))\n",
    "\n",
    "# Generate Sentence Embeddings Separately\n",
    "print(\"Generating Sentence Embeddings...\")\n",
    "t1 = time.time()\n",
    "docVecs =  generateSentenceVecs(rawSentences, sbert_model)\n",
    "tEmbedding = time.time() - t1\n",
    "print(\"Sentence Embeddings completed in \" + str(tEmbedding) + \"s\")\n",
    "\n",
    "## 3. CosRank\n",
    "print(\"Calculating CosRank...\")\n",
    "t1 = time.time()\n",
    "#docVecs =  generateSentenceVecs(rawSentences, sbert_model)\n",
    "cosSimMat = sentenceEmbeddingSimilarity(docVecs)\n",
    "cosRankScores = calcScoresTextRank(cosSimMat, rawSentences) \n",
    "cosRankSummaries = generateSummary(cosRankScores, N)\n",
    "\n",
    "rogueScore[\"CosRank\"] = calcRogue(cosRankSummaries, ref)\n",
    "execTime.append(time.time() - t1 + tEmbedding)\n",
    "#wordCountRatio.append(np.mean(rLength/np.array([len(d.split(\" \")) for d in cosRankSummaries])))\n",
    "wordCountRatio.append(rLength/np.array([len(d.split(\" \")) for d in cosRankSummaries]))\n",
    "\n",
    "## 4. Cluster Center\n",
    "print(\"Calculating Cluster Centres...\")\n",
    "t1 = time.time()\n",
    "#docVecs =  generateSentenceVecs(rawSentences, sbert_model)\n",
    "k = N\n",
    "np.random.seed(0)\n",
    "clusterCentres = calcClusterCentres(docVecs, rawSentences, k)\n",
    "clusterCentreSummaries = generateClusterSummaries(clusterCentres, N)\n",
    "\n",
    "rogueScore[\"ClusterCentre\"] = calcRogue(clusterCentreSummaries, ref)\n",
    "execTime.append(time.time() - t1 + tEmbedding)\n",
    "#wordCountRatio.append(np.mean(rLength/np.array([len(d.split(\" \")) for d in clusterCentreSummaries])))\n",
    "wordCountRatio.append(rLength/np.array([len(d.split(\" \")) for d in clusterCentreSummaries]))\n",
    "\n",
    "## 5. Cluster Rank\n",
    "print(\"Calculating ClusterRank...\")\n",
    "t1 = time.time()\n",
    "#trSimilarity = sentenceSimilarity(cleanWords)#, sentenceTokens)\n",
    "#textRankScores = calcScoresTextRank(trSimilarity, rawSentences)\n",
    "#docVecs =  generateSentenceVecs(rawSentences, sbert_model)\n",
    "\n",
    "np.random.seed(0)\n",
    "clusterCentres = calcClusterCentres(docVecs, rawSentences, k)\n",
    "#clusterRank = calcClusterRank(textRankScores, clusterCentres)\n",
    "clusterRank = calcClusterRank(cosRankScores, clusterCentres)\n",
    "clusterRankSummaries = generateClusterSummaries(clusterRank, N)\n",
    "\n",
    "rogueScore[\"ClusterRank\"] = calcRogue(clusterRankSummaries, ref)\n",
    "execTime.append(time.time() - t1 + tEmbedding)\n",
    "#wordCountRatio.append(np.mean(rLength/np.array([len(d.split(\" \")) for d in clusterRankSummaries])))\n",
    "wordCountRatio.append(rLength/np.array([len(d.split(\" \")) for d in clusterRankSummaries]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h1>Appendix</h1>\n",
    "The remaining cells can be used to explore and export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reference Summary:\n",
      "\n",
      "\n",
      "Showbiz party held in honour of a children's charity and featured a marmot\n",
      "\n",
      ". \n",
      "\n",
      "Russian celebrities happily posed with the creature as they arrived at event\n",
      "\n",
      ". \n",
      "\n",
      "But riot erupted when host announced he was to 'kill, cook and serve' it\n",
      "\n",
      ". \n",
      "\n",
      "Host said it would be 'entertaining to have a groundhog which didn't survive groundhog day'. The animal survived the ordeal and is now at zoo. \n",
      "\n",
      "\n",
      "TF-IDF:\n",
      "He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day'.He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day' .The groundhog day special cake.'I just thought it would be entertaining to have a groundhog who didn't actually survive groundhog day.'\n",
      "\n",
      "\n",
      "TextRank:\n",
      "A riot erupted at a .Groundhog Day party in Russia after guests realised their host was planning to cook a marmot - and then serve it with cranberry sauce.A riot erupted at a themed Groundhog Day party in Russia after guests realised their host was planning to cook a marmot - and then serve it with cranberry sauce .He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day'.He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day' .\n",
      "\n",
      "\n",
      "CosRank:\n",
      "A riot erupted at a .Groundhog Day party in Russia after guests realised their host was planning to cook a marmot - and then serve it with cranberry sauce.A riot erupted at a themed Groundhog Day party in Russia after guests realised their host was planning to cook a marmot - and then serve it with cranberry sauce .He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day'.But the guests were left with a very bad taste in their mouths when organiser Aleksey Polihun, 35, announced that he was about to kill and then cook the groundhog before serving it up on platter in a cranberry sauce .\n",
      "\n",
      "\n",
      "ClusterCenter:\n",
      "'The poor thing was terrified... and it may be hard to believe but some people actually cheered him on.'He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day'.He said he wanted to kill and cook it as he thought it would be 'entertaining to have a groundhog which didn't survive groundhog day' .One of the guests in the end offered to buy the animal off the menu and took it back to the zoo .\n",
      "\n",
      "\n",
      "ClusterRank:\n",
      "A host of Russian celebrities happily posed with the creature, a relative of the squirrel, as they arrived at the charity event in aid of underprivileged children .A riot erupted at a .Groundhog Day party in Russia after guests realised their host was planning to cook a marmot - and then serve it with cranberry sauce.A riot erupted at a themed Groundhog Day party in Russia after guests realised their host was planning to cook a marmot - and then serve it with cranberry sauce .The showbiz bash - held in Moscow - featured the marmot, which had been brought in as 'the star of the show' from a local children's petting zoo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iFile=8\n",
    "print(\"\\nReference Summary:\\n\" + dmData[\"Summary\"][iFile] + \"\\n\")\n",
    "print(\"\\nTF-IDF:\\n\" + tfIdfSummaries[iFile] + \"\\n\")\n",
    "print(\"\\nTextRank:\\n\" + textRankSummaries[iFile] + \"\\n\")\n",
    "print(\"\\nCosRank:\\n\" + cosRankSummaries[iFile] + \"\\n\")\n",
    "print(\"\\nClusterCenter:\\n\" + clusterCentreSummaries[iFile] + \"\\n\")\n",
    "print(\"\\nClusterRank:\\n\" + clusterRankSummaries[iFile] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Data\n",
    "df2 = pd.DataFrame([execTime, np.mean(wordCountRatio,axis=1)], columns=['TF-IDF','TextRank','CosRank', 'ClusterCentre', 'ClusterRank'], index=['execTime','wordCountRatio'])\n",
    "summaryDf = pd.concat([rogueScore, df2])\n",
    "#summaryDf\n",
    "#summaryDf.to_excel('tmp.xlsx')\n",
    "#dmData.to_excel('tmp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move 1000 files\n",
    "import os\n",
    "from shutil import copyfile\n",
    "sourceFolder = \"/home/dclabby/Documents/DataScience/Data Sets/dailymail_stories/dailymail/stories/\"\n",
    "destFolder = \"/home/dclabby/Documents/Springboard/HDAIML_SEP/Semester02/ArtIntel/Project/TextSummCode/data/\"\n",
    "\n",
    "folderContents = os.listdir(sourceFolder)\n",
    "nFiles = 1000\n",
    "files = folderContents[0:nFiles]\n",
    "\n",
    "for iFile in files:\n",
    "    copyfile(sourceFolder + iFile, destFolder + iFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
